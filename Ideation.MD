# **연구 주제 선정을 위한 브레인 스토밍**

## **딥러닝 활용한 양자컴퓨팅의 노이즈 완화**
### **1. 핵심 키워드 (Keywords)**

- **양자컴퓨팅(Quantum Computing)**: GPU 이후 차세대 연산 패러다임으로, 병렬성과 고속 연산을 통해 AI 발전을 견인할 수 있는 잠재력을 지닌 기술
- **양자 노이즈(Quantum Noise)**: 현재 양자컴퓨터의 가장 큰 한계 중 하나로, 큐비트가 외부 환경과 상호작용하며 오류가 발생하는 문제가 있음
- **딥러닝 기반 노이즈 완화(Noise Mitigation with Deep Learning)**: Autoencoder, VAE 등 딥러닝 기법을 활용하여 노이즈가 포함된 양자 상태 데이터를 복원하고, 정확한 계산 결과를 도출하는 접근.


### **2. 문제 정의 (Problem Definition)**

- **누구를 위해**:
    
    양자컴퓨팅 연구자, 양자 소프트웨어 개발자, 차세대 AI 연구자들을 위해.
    
- **어떤 문제를 해결하는가**:
    
    현재 양자컴퓨터는 **노이즈로 인한 오류** 때문에 정확한 연산이 어려움.
    
    예를 들어, 단순한 2~4큐비트 연산조차 Depolarizing, Amplitude damping, Phase damping 등 다양한 노이즈가 개입하면 계산 결과가 왜곡됩니다. 기존의 물리적 오류 보정 방식은 많은 오버헤드와 자원을 요구하므로, 학부 수준의 연구에서 접근하기 어렵습니다. 따라서 **시뮬레이션 환경에서 노이즈 데이터를 생성하고, 이를 딥러닝 기법으로 복원**하는 방법을 실험적으로 검증할 필요가 있습니다.
    


### **3. 해결 방안 및 목표 (Solution & Goal)**

- **어떤 기술을 사용해서**:
    - **Qiskit 시뮬레이터**: 2~4큐비트 회로를 설계하고 Depolarizing, Amplitude damping, Phase damping 노이즈를 적용하여 데이터 생성.
    - **딥러닝(Autoencoder, VAE)**: 노이즈가 포함된 양자 상태 데이터를 학습하여 원래 상태를 복원.
    - **성능 지표**: Fidelity, Trace distance 등을 활용하여 복원 성능을 정량적으로 평가.
    - **기존 기법 비교**: 여유가 된다면 Zero-Noise Extrapolation(ZNE) 등 기존 노이즈 완화 기법과 비교 실험.
- **무엇을 만들려고 하는가**:
    - 노이즈가 포함된 양자 데이터셋을 생성하는 **양자 회로 시뮬레이션 모듈**.
    - 딥러닝 기반으로 노이즈 데이터를 복원하는 **Autoencoder 모델**.
    - 복원 전/후의 결과를 Fidelity, Trace distance로 비교·분석할 수 있는 **평가 및 시각화 툴**.
    - 이를 통해 “딥러닝 기반 노이즈 완화가 기존 기법 대비 어떤 가능성을 지니는지”를 학부 수준에서 실험적으로 검증하는 것을 목표로 함.

---

## **딥러닝을 활용한 렌더링 이미지의 디노이징**
### 1. 핵심 키워드

- **렌더링 노이즈**: 몬테카를로 경로 추적 기반 렌더링에서 샘플 수가 부족할 때 발생하는 잡음
    (몬테카를로 경로 추적: 현실적인 조명 효과를 계산하기 위해 사용하는 컴퓨터 그래픽스 렌더링 기법. 빛의 경로 중 일부만 무작위로 추적하고 평균을 내어 기댓값 형태로 실제 조명을 근사한다.)
- **딥러닝 기반 디노이징**: 신경망을 활용하여 노이즈를 줄이고, 고품질 이미지를 복원하는 기법
- **데이터 기반 학습**: 노이즈 이미지와 정답(노이즈가 제거된 이미지) 쌍을 학습
- **실시간 렌더링**: 빠른 속도로 디노이징을 수행하여 게임·VR/AR 등 인터랙티브 환경에서도 활용 가능



### 2. 문제 정의

- **문제 상황**:
    
    기존의 글로벌 일루미네이션이나 경로 추적 기반 렌더링은 사실적 이미지를 생성하지만, 계산량이 매우 커서 한 장의 이미지를 얻는 데도 긴 시간이 걸림.
    따라서 샘플 수를 줄이면 연산량은 줄어들지만, 결과물에 **랜더링 노이즈**가 발생하게 됨.
    
- **문제점**:
    - 고품질 이미지를 얻기 위해선 많은 샘플 → 연산 시간 급증
    - 적은 샘플로 빠른 결과를 얻으려 하면 노이즈 발생 → 시각적 품질 저하
    - 필터링 기법과 같은 기존의 비딥러닝 기반 디노이징은 세부 디테일이 손실되거나 아티팩트가 생기는 한계



### 3. 해결 방안 및 목표

- **해결 방안**:
    - 딥러닝 모델(CNN, U-Net, Transformer…)을 활용하여 노이즈가 포함된 렌더링 이미지를 입력받고, 대응되는 고품질 이미지를 학습 데이터로 삼아 **노이즈 제거 모델**을 학습.
    - 학습된 모델을 통해 **적은 샘플로도 고품질 렌더링**을 재현할 수 있음.
- **목표**:
    1. **렌더링 효율성 향상**: 고샘플 렌더링 대비 훨씬 적은 연산으로 유사한 품질 확보
    2. **시각적 품질 보장**: 노이즈 제거 과정에서도 디테일 보존 및 아티팩트 최소화
    3. **실시간성 확보**: 게임 엔진, VR/AR, 영화 제작 파이프라인 등에서 실시간/인터랙티브 사용 가능
 
---
 
## **멀티모달 엣지 AI 기반 실시간 예지 정비 시스템**
### **1. 핵심 키워드 (Keywords)**

- **멀티모달 AI**: 영상, 음향, 시계열 데이터를 통합적으로 분석하여 기계 상태를 진단합니다.
- **엣지 컴퓨팅**: 데이터를 클라우드로 전송하지 않고 기계 옆에서 실시간으로 처리하여 지연 시간을 최소화합니다.
- **예지 정비(Predictive Maintenance)**: 기계 고장이 발생하기 전에 미리 예측하여 정비함으로써 생산성 손실을 막습니다.



### **2. 문제 정의 (Problem Definition)**

- **누구를 위해**: 공장 자동화 시스템을 관리하는 **제조업체 관리자 및 현장 작업자**를 위해.
- **어떤 문제를 해결하는가**: 기존의 센서 데이터만으로는 설비의 미세한 이상 징후를 정확히 파악하기 어렵습니다. 예를 들어, 기계의 미세한 소음이나 눈에 잘 보이지 않는 진동은 센서만으로 감지하기 힘들며, 고장이 발생한 후에야 문제를 알게 되어 생산 라인이 멈추는 **비용과 시간 손실**이 발생합니다.



### **3. 해결 방안 및 목표 (Solution & Goal)**

- **어떤 기술을 사용해서**:
    - **멀티모달 딥러닝**: 진동 센서의 시계열 데이터, 기계음을 녹음한 오디오 데이터, 기계 상태를 촬영한 영상 데이터를 융합하여 기계의 건강 상태를 진단하는 모델을 개발합니다.
    - **생성 AI**: 학습 데이터가 부족할 경우, 정상 상태의 센서 및 오디오 데이터를 기반으로 다양한 고장 상황의 합성 데이터를 생성하여 모델을 더 효과적으로 훈련시킵니다.
    - **엣지 AI**: 클라우드가 아닌 현장 엣지 디바이스에서 실시간으로 데이터를 분석하고, 이상 징후를 발견하면 즉시 작업자에게 알림을 보냅니다.
- **무엇을 만들려고 하는가**:
    - 센서, 마이크, 카메라가 통합된 **멀티모달 데이터 수집 모듈**.
    - 수집된 데이터를 실시간으로 분석하여 기계의 잔존 수명(RUL)과 고장 발생 가능성을 예측하는 **통합 예지 정비 시스템**.
    - 이 시스템을 통해 고장이 발생하기 전에 미리 경고를 보내고, 어떤 징후(예: "특정 주파수의 소음 증가")가 문제의 원인인지 설명해주는 **시각화 대시보드**.

---

## **강화학습 기반 샘플링과 뉴럴 노이즈 예측을 통한 랜더링링 최적화**

### **1. 핵심 키워드 (Keywords)**

- **렌더링 최적화(Rendering Optimization)**: Path Tracing, Monte Carlo 샘플링 효율 개선
- **강화학습(Reinforcement Learning)**: 중요 영역에 적응적 샘플링 전략 학습
- **뉴럴 렌더링(Neural Rendering)**: 딥러닝을 활용한 샘플 보간 및 노이즈 제거
- **실시간 최적화(Real-Time Rendering)**: 품질–연산 속도 균형 추구



### **2. 문제 정의 (Problem Definition)**

- **어떤 문제를 해결하는가**:
    -Path Tracing은 현실감 있는 이미지를 생성할 수 있지만, 고품질 출력을 얻기 위해서는 수천~수만 개의 샘플이 필요해 연산 비용이 크다.
    -Monte Carlo 샘플링은 무작위성에 의존하기 때문에 샘플 수가 적을 경우 심각한 노이즈가 발생한다.
    -기존 CNN 기반 denoiser는 노이즈 제거는 가능하지만, 샘플링 전략 자체를 최적화하지 못해 근본적인 비효율성이 남아 있다.




### **3. 해결 방안 및 목표 (Solution & Goal)**

- **어떤 기술을 사용해서**:
    - 1. 강화학습 기반 적응적 샘플링
    에이전트가 장면의 픽셀·패스를 탐색하며 “샘플을 집중할 영역”을 학습.
    보상 함수: 이미지 품질 지표(PSNR, SSIM) – 연산 시간.



    - 2. 딥러닝 기반 노이즈 예측 모델
    저샘플 이미지를 입력받아 고샘플 ground truth를 예측하도록 CNN/UNet 모델 학습.
    RL 샘플링과 결합해 샘플 효율과 노이즈 억제를 동시에 달성.



    - 3. 하이브리드 최적화 기법
    Early stopping: 충분히 학습된 픽셀의 샘플링을 중단.
    Adaptive step size: 장면 복잡도에 따라 샘플 수를 동적으로 조절.




- **무엇을 만들려고 하는가**:
    실험용 렌더링 프레임워크: PBRT 또는 Mitsuba 렌더러에 RL 모듈을 결합해 성능 비교.
    비교 실험:
    Baseline ①: 기존 Monte Carlo Path Tracing
    Baseline ②: CNN 기반 denoiser 적용
    제안: RL 기반 adaptive sampling + 노이즈 예측 모델

    측정 지표: PSNR, SSIM, 렌더링 시간, 시각적 노이즈 수준


# **연구 주제 선정 및 구체화**
## **딥러닝을 이용한 양자컴퓨팅 오류 완화(Quantum Error Mitigation)**

## 1. 핵심 키워드

- **양자 컴퓨팅 (Quantum Computing)**
- **노이즈 완화 (Error Mitigation, QEM)**
- **머신러닝 기반 오류 보정 (ML-based Error Correction)**
- **딥러닝 (Autoencoder, Variational Autoencoder, Transformer)**
- **전이 학습 (Transfer Learning)**
- **효율적 미세조정 (PEFT, LoRA)**
- **시뮬레이터–실데이터 브릿징 (Simulator-to-Hardware Bridging)**

---

## 2. 문제 정의

1. **양자 컴퓨터의 가능성**
    - 양자 얽힘과 중첩을 활용해 고전적으로 풀기 어려운 문제를 효율적으로 해결할 잠재력이 있음.
    - 화학적 분자 시뮬레이션, 최적화, 암호 해독 등 다양한 응용 가능성.
2. **NISQ 시대의 한계**
    - 현재 양자 하드웨어는 수십~수백 큐빗 규모로 확장됐으나, **노이즈**가 계산의 신뢰성을 크게 저해함.
    - 대표적 오류:
        - **decoherence** → 상태 붕괴
        - **gate error** → 연산 불완전성
        - **readout noise** → 최종 결과 왜곡
3. **기존 오류 완화 기법의 한계**
    - **Calibration matrix**: 노이즈 행렬 추정 → 역행렬 보정, 그러나 큐빗 수 늘면 행렬 크기가 지수적으로 커짐.
    - **ZNE (Zero-Noise Extrapolation)**: 노이즈 스케일링 후 외삽 → 샘플링 오버헤드 큼.
    - **PEC (Probabilistic Error Cancellation)**: 원리적으로 정확하나 샘플링 복잡도가 지수적.
    - 공통적으로 **비용이 크고, 일반화가 어려움**.

---

## 3. 연구 해결 방안 및 목표

### (1) 연구 접근 방식

1. **시뮬레이터 기반 선학습**
    - Qiskit의 Fake backends (예: FakeLima, FakeAlgiers, FakeBrisbane)를 활용해 **noisy–ideal 기대값 쌍**을 생성.
    - 다양한 회로 구조(랜덤 회로, Trotterized 회로 등)와 노이즈 조건(incoherent, readout, coherent)을 반영.
    - Autoencoder/VAE 모델이 noisy 데이터를 latent space에 압축 후 ideal 데이터로 복원 → **노이즈 제거 representation 학습**.
2. **실제 하드웨어 데이터로 전이학습**
    - IBM Quantum 장치(예: ibm_algiers, ibm_brisbane) 또는 오픈 데이터셋에서 실제 노이즈 데이터를 수집.
    - 시뮬레이터에서 학습한 모델을 전이학습(transfer learning)으로 적은 실제 데이터에 맞게 fine-tuning.
    - 학습 전략:
        - **feature extractor 고정 + 출력 계층만 학습** → 빠른 수렴, 적은 데이터로 가능.
        - **PEFT(LoRA)** 적용 → 거대한 모델 전체가 아니라 작은 low-rank 모듈만 조정, 자원 효율성 극대화.
3. **모델 효율화 및 평가**
    - **성능 지표**: KL divergence, TVD(Total Variation Distance), accuracy, 에너지 추정 오차(VQE).
    - **비교 baseline**: ZNE, calibration matrix.
    - **학습 효율성**: 필요한 회로 수, 샷 수, 학습 파라미터 수를 기존 기법과 비교.

---

### (2) 구체적 연구 목표

1. **노이즈 완화 성능 향상**
    - 기존 ZNE 대비 더 낮은 오차, 더 적은 오버헤드로 이상적 기대값 근사.
2. **범용적 적용 가능성**
    - 서로 다른 하드웨어(백엔드)와 회로 클래스에 일반화 가능한 프레임워크 구축.
3. **데이터·자원 효율적 접근**
    - 제한된 실험 데이터와 연산 자원에서도 빠르게 fine-tuning 가능한 경량 모델 제안.

---

## 4. 기대 효과

- **실제 양자 시스템 활용 강화**
    - NISQ 장비에서 신뢰도 높은 양자 알고리즘 실행 가능 → 화학 시뮬레이션, VQE, QAOA 등 응용.
- **데이터 수집 비용 절감**
    - 시뮬레이터 선학습 + 전이학습 구조 → 실제 장치에서 대규모 데이터 수집 불필요.
- **자원 효율성 제고**
    - LoRA 등 PEFT 적용으로 학습·추론 비용을 크게 절감.
- **양자–머신러닝 융합 연구 기여**
    - 물리 기반 QEM을 보완하면서, ML 기반 노이즈 보정의 실용적 가능성을 제시.
